{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import collections\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from vocabulary import Vocabulary\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "START_TOKEN = \"^\"\n",
    "END_TOKEN = \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "TRAIN_PROP = 0.7\n",
    "VAL_PROP = 0.15\n",
    "TEST_PROP = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/amazon_train_small.csv\", header=None)\n",
    "df.columns = ['label', 'title', 'body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Right on the money</td>\n",
       "      <td>We are using the this book to get 100+ certifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Serves its Purpose!</td>\n",
       "      <td>Couldn't go without it. My 3 1/2 year still we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Trailer Park Bwoys!!!</td>\n",
       "      <td>we get to see it on paramount in ol' LND UK an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>buyer beware</td>\n",
       "      <td>There are companies selling Bosch knock-offs o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Great for those cold winters</td>\n",
       "      <td>If you are looking to keep your water liquifie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>keeps breaking!</td>\n",
       "      <td>I own a Nomad II 64 MP3 player and it has brok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Not Happy</td>\n",
       "      <td>Thought this was in English but it is in Germa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>mount doesn't stay put</td>\n",
       "      <td>I saw quite a few very positive reviews for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>Finally , Some Common Sense!</td>\n",
       "      <td>I was afraid this book would just bash media, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>Good value, time saver</td>\n",
       "      <td>My wife is a lifelong weightwatcher. She has b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>Shyamalan bested!</td>\n",
       "      <td>Shyamalan was at some point being credited wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>My first Sunn O))) experience</td>\n",
       "      <td>This is the first CD I've bought from this ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>Cannot get this product set up.</td>\n",
       "      <td>I received this router today and have spent 3 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>Could be great but...</td>\n",
       "      <td>Who would have thought that someone could cram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>Not good at all</td>\n",
       "      <td>This is probably the worst design for an egg p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>Disturbing and compelling</td>\n",
       "      <td>Like a rotted tooth or a troubling sore I retu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>Unintentional camp and a very bad film!</td>\n",
       "      <td>Now we all know at this point that Tom Cruise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>bitchen</td>\n",
       "      <td>My dad found this book for 3 payments of 28 do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>Interesting Book</td>\n",
       "      <td>The internet sites out there that talk about w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>Alice!!!! What happened to YOU AND ME?</td>\n",
       "      <td>Wonderful sound, excellent video.... but, but,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                    title  \\\n",
       "0       2                       Right on the money   \n",
       "1       2                      Serves its Purpose!   \n",
       "2       2                    Trailer Park Bwoys!!!   \n",
       "3       1                             buyer beware   \n",
       "4       2             Great for those cold winters   \n",
       "5       1                          keeps breaking!   \n",
       "6       1                                Not Happy   \n",
       "7       1                   mount doesn't stay put   \n",
       "8       2             Finally , Some Common Sense!   \n",
       "9       2                   Good value, time saver   \n",
       "10      1                        Shyamalan bested!   \n",
       "11      2            My first Sunn O))) experience   \n",
       "12      1          Cannot get this product set up.   \n",
       "13      1                    Could be great but...   \n",
       "14      1                          Not good at all   \n",
       "15      2                Disturbing and compelling   \n",
       "16      1  Unintentional camp and a very bad film!   \n",
       "17      2                                  bitchen   \n",
       "18      2                         Interesting Book   \n",
       "19      2   Alice!!!! What happened to YOU AND ME?   \n",
       "\n",
       "                                                 body  \n",
       "0   We are using the this book to get 100+ certifi...  \n",
       "1   Couldn't go without it. My 3 1/2 year still we...  \n",
       "2   we get to see it on paramount in ol' LND UK an...  \n",
       "3   There are companies selling Bosch knock-offs o...  \n",
       "4   If you are looking to keep your water liquifie...  \n",
       "5   I own a Nomad II 64 MP3 player and it has brok...  \n",
       "6   Thought this was in English but it is in Germa...  \n",
       "7   I saw quite a few very positive reviews for th...  \n",
       "8   I was afraid this book would just bash media, ...  \n",
       "9   My wife is a lifelong weightwatcher. She has b...  \n",
       "10  Shyamalan was at some point being credited wit...  \n",
       "11  This is the first CD I've bought from this ban...  \n",
       "12  I received this router today and have spent 3 ...  \n",
       "13  Who would have thought that someone could cram...  \n",
       "14  This is probably the worst design for an egg p...  \n",
       "15  Like a rotted tooth or a troubling sore I retu...  \n",
       "16  Now we all know at this point that Tom Cruise ...  \n",
       "17  My dad found this book for 3 payments of 28 do...  \n",
       "18  The internet sites out there that talk about w...  \n",
       "19  Wonderful sound, excellent video.... but, but,...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonReviewsDataset(object):\n",
    "    \"\"\"Amazon Reviews text dataset for language modeling.\n",
    "    \n",
    "       Args:\n",
    "        data_path (str): Path to Amazon reviews data file.\n",
    "        num_samples (int): Number of amazon reviews to load.\n",
    "        max_review_length (int): Filters reviews longer than specified length.\n",
    "            [default=400]\n",
    "        max_sequence_length (int): Max length of sequences for use in training language model.\n",
    "            [default=40]\n",
    "        sentiment (int): sentiment of reviews to select, 1 (negative) or 2 (positive).\n",
    "            [default=2]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        data = pd.read_csv(data_path, names=['sentiment', 'title', 'review'])\n",
    "        self.data = self.preprocess(data)\n",
    "        \n",
    "    def preprocess(self, review_df):\n",
    "        def _preprocess_func(text):\n",
    "            text = text.lower()\n",
    "            text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "            text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "            return text\n",
    "        \n",
    "        # Splitting the subset by sentiment to create our new train, val, and test splits\n",
    "        by_sentiment = collections.defaultdict(list)\n",
    "        for _, row in review_df.iterrows():\n",
    "            by_sentiment[row.sentiment].append(row.to_dict())\n",
    "\n",
    "        final_list = []\n",
    "        np.random.seed(SEED)\n",
    "\n",
    "        for _, item_list in sorted(by_sentiment.items()):\n",
    "\n",
    "            np.random.shuffle(item_list)\n",
    "\n",
    "            n_total = len(item_list)\n",
    "            n_train = int(TRAIN_PROP * n_total)\n",
    "            n_val = int(VAL_PROP * n_total)\n",
    "            n_test = int(TEST_PROP * n_total)\n",
    "\n",
    "            # Give data point a split attribute\n",
    "            for item in item_list[:n_train]:\n",
    "                item['split'] = 'train'\n",
    "\n",
    "            for item in item_list[n_train:n_train+n_val]:\n",
    "                item['split'] = 'val'\n",
    "\n",
    "            for item in item_list[n_train+n_val:n_train+n_val+n_test]:\n",
    "                item['split'] = 'test'\n",
    "            \n",
    "            # Add to final list\n",
    "            final_list.extend(item_list)\n",
    "        \n",
    "        output_df = pd.DataFrame(final_list)\n",
    "        output_df['review'] = output_df.review.apply(_preprocess_func)\n",
    "        return output_df\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "class AmazonReviewsVectorizer(object):\n",
    "    def __init__(self, word_vocab, max_seq_length):\n",
    "        self.word_vocab = word_vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    @classmethod\n",
    "    def fit(cls, review_df):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        vocab = Vocabulary(use_unks=False,\n",
    "                           use_start_end=True,\n",
    "                           use_mask=True,\n",
    "                           start_token=START_TOKEN,\n",
    "                           end_token=END_TOKEN)\n",
    "        max_seq_length = 0\n",
    "        for review in review_df['review'].values:\n",
    "            review_split = review.split(\" \")\n",
    "            for word in review_split:\n",
    "                vocab.add(word)\n",
    "            if len(review_split) > max_seq_length:\n",
    "                max_seq_length = len(review_split)\n",
    "        max_seq_length += 2\n",
    "        return cls(vocab, max_seq_length)\n",
    "\n",
    "    def transform(self, review_df, split='train'):\n",
    "        review_df = review_df[review_df.split==split].reset_index()\n",
    "        num_data = len(review_df)\n",
    "        \n",
    "        x_words = np.zeros((num_data, self.max_seq_length), dtype=np.int64)\n",
    "        y_sentiment = np.zeros(num_data, dtype=np.int64)\n",
    "\n",
    "        for index, row in review_df.iterrows():\n",
    "            x_indices = list(self.word_vocab.map(row['review'].split(' '), include_start_end=True))\n",
    "            x_words[index, :len(x_indices)] = x_indices \n",
    "            y_sentiment[index] = row['sentiment']\n",
    "            \n",
    "        return VectorizedAmazonReviews(x_words, y_sentiment)\n",
    "\n",
    "class VectorizedAmazonReviews(Dataset):\n",
    "    def __init__(self, x_input, y_target):\n",
    "        self.x_input = x_input\n",
    "        self.y_target = y_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_input)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'x_input': self.x_input[index],\n",
    "                'y_target': self.y_target[index],\n",
    "                'x_lengths': len(self.x_input[index].nonzero()[0])}\n",
    "    \n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AmazonReviewsDataset(\"../data/amazon_train_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = AmazonReviewsVectorizer.fit(dataset.get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = vectorizer.transform(dataset.get_data(), split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = vectorizer.transform(dataset.get_data(), split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dict = next(generate_batches(train_dataset, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK\n",
    "\n",
    "1. create an embedding layer and get it to work with the batch_dict above\n",
    "2. use either:\n",
    "    1. a deep averaging network\n",
    "    2. a convnet\n",
    "    3. a RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch04",
   "language": "python",
   "name": "pytorch04"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
